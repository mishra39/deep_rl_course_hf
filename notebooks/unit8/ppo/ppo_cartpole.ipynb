{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Create a virtual display ğŸ”½\n",
        "\n",
        "During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n",
        "\n",
        "Hence the following cell will install the librairies and create and run a virtual screen ğŸ–¥"
      ],
      "metadata": {
        "id": "bTpYcVZVMzUI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jV6wjQ7Be7p5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay\n",
        "!apt install x11-utils\n",
        "!pip install pyglet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "ww5PQH1gNLI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3f6811e-354d-4f31-d1fd-a8f1ec4206ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7c00f1eeab40>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display video\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "def show_video():\n",
        "    \"\"\"Embeds the recorded video in the notebook output.\"\"\"\n",
        "    mp4list = glob.glob('videos/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        video = mp4list[0]\n",
        "        with io.open(video, 'r+b') as f:\n",
        "            encoded = base64.b64encode(f.read()).decode()\n",
        "        # Create an HTML display object for Colab\n",
        "        return HTML(data=f'<video width=\"1000\" controls><source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\" /></video>')\n",
        "    else:\n",
        "        print(\"No video files found in the 'videos' directory.\")"
      ],
      "metadata": {
        "id": "7js6sBTwmEg_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies ğŸ”½\n",
        "\n",
        "The first step is to install the dependencies, weâ€™ll install multiple ones:\n",
        "- `gymnasium`\n",
        "- `panda-gym`: Contains the robotics arm environments.\n",
        "- `stable-baselines3`: The SB3 deep reinforcement learning library.\n",
        "- `huggingface_sb3`: Additional code for Stable-baselines3 to load and upload models from the Hugging Face ğŸ¤— Hub.\n",
        "- `huggingface_hub`: Library allowing anyone to work with the Hub repositories.\n",
        "\n",
        "â² The installation can **take 10 minutes**."
      ],
      "metadata": {
        "id": "e1obkbdJ_KnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install stable-baselines3[extra]\n",
        "!pip install gymnasium"
      ],
      "metadata": {
        "id": "TgZUkjKYSgvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53aa549f-867c-4596-80b7-afb36da17c01"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install huggingface_sb3\n",
        "# !pip install huggingface_hub\n",
        "# !pip install panda_gym"
      ],
      "metadata": {
        "id": "ABneW6tOSpyU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6BLqKndoR1t"
      },
      "source": [
        "## W&B Prerequisites\n",
        "\n",
        "Install the W&B Python SDK and log in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ImmYONLQoR1u"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qU\n",
        "!pip install -q gym numpy tensorboard\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xjomxlWXoR1w"
      },
      "outputs": [],
      "source": [
        "# Log in to your W&B account\n",
        "import wandb\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_cQ5-08BoR1x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee43ae3-6227-46dc-fd11-4de15b663bea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmishra39\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "h-cj5rkzpwGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from distutils.util import strtobool\n",
        "\n",
        "import gymnasium as gym  # Use gymnasium instead of gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "W2bqN8jzp564"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arguments"
      ],
      "metadata": {
        "id": "GkxawpDv85Nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "@dataclass\n",
        "class PPOConfig:\n",
        "    # Experiment settings\n",
        "    exp_name: str = \"ppo_experiment\"\n",
        "    gym_id: str = \"CartPole-v1\"\n",
        "    learning_rate: float = 2.5e-4\n",
        "    seed: int = 1\n",
        "    total_timesteps: int = 25000\n",
        "    torch_deterministic: bool = True\n",
        "    cuda: bool = True\n",
        "    track: bool = False\n",
        "    wandb_project_name: str = \"ppo-implementation-details\"\n",
        "    wandb_entity: Optional[str] = None\n",
        "    capture_video: bool = False\n",
        "\n",
        "    # Algorithm specific arguments\n",
        "    num_envs: int = 4\n",
        "    num_steps: int = 128\n",
        "    anneal_lr: bool = True\n",
        "    gae: bool = True\n",
        "    gamma: float = 0.99\n",
        "    gae_lambda: float = 0.95\n",
        "    num_minibatches: int = 4\n",
        "    update_epochs: int = 4\n",
        "    norm_adv: bool = True\n",
        "    clip_coef: float = 0.2\n",
        "    clip_vloss: bool = True\n",
        "    ent_coef: float = 0.01\n",
        "    vf_coef: float = 0.5\n",
        "    max_grad_norm: float = 0.5\n",
        "    target_kl: Optional[float] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # Computed values\n",
        "        self.batch_size = int(self.num_envs * self.num_steps)\n",
        "        self.minibatch_size = int(self.batch_size // self.num_minibatches)\n",
        "\n",
        "# Create instance with default values\n",
        "# args = PPOConfig()\n",
        "\n",
        "# Or customize specific values\n",
        "# args = PPOConfig(learning_rate=1e-3, num_envs=8, total_timesteps=50000)\n",
        "\n",
        "# print(f\"Batch size: {args.batch_size}\")\n",
        "# print(f\"Minibatch size: {args.minibatch_size}\")"
      ],
      "metadata": {
        "id": "6hwqBmy387R9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gym Envrionment"
      ],
      "metadata": {
        "id": "HTO4Ho3z9EBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def make_env(gym_id, seed, idx, capture_video, run_name):\n",
        "#     def thunk():\n",
        "#       env = gym.make(gym_id, render_mode=\"rgb_array\" if capture_video else None)\n",
        "#       env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "#       if capture_video:\n",
        "#         env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\", episode_trigger=lambda episode_id: True,  # we'll recreate env only on selected episodes\n",
        "#             disable_logger=True,  # silence ffmpeg logs) # added to avoid conflict with wandb logger\n",
        "#       env.action_space.seed(seed)\n",
        "#       env.observation_space.seed(seed)\n",
        "#       return env\n",
        "\n",
        "#     return thunk\n",
        "\n",
        "def make_env(env_id, seed, idx, capture_video, run_name):\n",
        "    def thunk():\n",
        "        # base env with render_mode for video\n",
        "        env = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "\n",
        "        if capture_video and idx == 0:\n",
        "            video_folder = os.path.join(config.video_dir, run_name)\n",
        "            env = gym.wrappers.RecordVideo(\n",
        "                env,\n",
        "                video_folder=video_folder,\n",
        "                name_prefix=f\"{env_id}__{run_name}\",\n",
        "                episode_trigger=lambda episode_id: True,  # Record every episode for that env\n",
        "                disable_logger=True,\n",
        "            )\n",
        "        env.action_space.seed(seed)\n",
        "        env.observation_space.seed(seed)\n",
        "        return env\n",
        "\n",
        "    return thunk"
      ],
      "metadata": {
        "id": "GclrE21F9MZ9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Initialization"
      ],
      "metadata": {
        "id": "PqwnPqULZ9Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "  torch.nn.init.orthogonal_(layer.weight, std)\n",
        "  torch.nn.init.constant_(layer.bias, bias_const)\n",
        "  return layer"
      ],
      "metadata": {
        "id": "KN_khmRUbdG1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Loop"
      ],
      "metadata": {
        "id": "C5nZUu1b92zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = PPOConfig(track=True, capture_video=True)\n",
        "run_name = f\"{args.gym_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
        "if args.track:\n",
        "    # wandb.init(\n",
        "    #     project=args.wandb_project_name,\n",
        "    #     entity=args.wandb_entity,\n",
        "    #     sync_tensorboard=True,\n",
        "    #     config=vars(args),\n",
        "    #     name=run_name,\n",
        "    #     monitor_gym=False, # Set to False to prevent conflict with RecordVideo\n",
        "    #     save_code=True,\n",
        "    # )\n",
        "\n",
        "    run = wandb.init(\n",
        "    project=args.wandb_project_name,\n",
        "    name=run_name,\n",
        "    config=dict(\n",
        "        env_id=args.gym_id,\n",
        "        total_timesteps=args.total_timesteps,\n",
        "        num_envs=args.num_envs,\n",
        "        video_dir=\"videos\",\n",
        "        video_eval_dir=\"videos_eval\",\n",
        "        train_video_every_n_episodes=50,\n",
        "        eval_episodes=5,\n",
        "    ),\n",
        ")\n",
        "    config = wandb.config\n",
        "\n",
        "writer = SummaryWriter(f\"runs/{run_name}\")\n",
        "writer.add_text(\n",
        "    \"hyperparameters\",\n",
        "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
        ")\n",
        "\n",
        "# TRY NOT TO MODIFY: seeding\n",
        "random.seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "iLgY25BJ94qq",
        "outputId": "2ccd9de6-238a-4a9e-f880-886d460374f4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>charts/SPS</td><td>â–â–‚â–ƒâ–ƒâ–ƒâ–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ</td></tr><tr><td>charts/episodic_length</td><td>â–â–â–ƒâ–‚â–‚â–â–â–â–â–â–â–‚â–‚â–â–‚â–„â–‚â–‚â–â–â–ƒâ–ƒâ–‚â–‚â–„â–â–‚â–ƒâ–„â–â–ƒâ–ƒâ–„â–†â–ƒâ–ˆâ–†â–‚â–„â–ƒ</td></tr><tr><td>charts/episodic_return</td><td>â–â–â–‚â–‚â–‚â–â–‚â–‚â–â–‚â–‚â–‚â–â–‚â–â–ƒâ–‚â–‚â–â–‚â–‚â–„â–‚â–…â–â–…â–ˆâ–„â–â–‚â–ƒâ–‚â–‚â–ˆâ–‚â–…â–„â–ˆâ–ˆâ–‚</td></tr><tr><td>charts/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–†â–†â–…â–„â–„â–…â–…â–…â–…â–…â–…â–…â–…â–„â–„â–‚â–‚â–„â–ƒâ–ƒâ–ƒâ–‚â–â–â–</td></tr><tr><td>global_step</td><td>â–â–â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆ</td></tr><tr><td>losses/approx_kl</td><td>â–†â–†â–†â–‡â–ˆâ–‡â–„â–‚â–‚â–‚â–‚â–â–â–‚â–‚â–‚â–‚â–â–â–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>losses/clipfrac</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>losses/entropy</td><td>â–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–…â–„â–…â–…â–„â–„â–„â–ƒâ–…â–„â–…â–„â–„â–‚â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–„â–ƒâ–„â–„â–ƒâ–ƒâ–ƒâ–‚â–ƒâ–„â–ƒ</td></tr><tr><td>losses/explained_variance</td><td>â–â–‚â–â–‚â–â–â–‚â–â–‚â–â–â–‚â–â–‚â–â–‚â–â–ƒâ–ƒâ–‚â–‚â–â–„â–ƒâ–ƒâ–‚â–‚â–ƒâ–‚â–‚â–‚â–„â–‚â–ƒâ–„â–ˆâ–ƒâ–‡â–„â–„</td></tr><tr><td>losses/old_approx_kl</td><td>â–„â–„â–‚â–„â–„â–†â–‡â–†â–„â–ˆâ–‚â–ƒâ–ƒâ–â–…â–„â–ƒâ–ƒâ–„â–„â–â–ƒâ–„â–„â–„â–„â–ƒâ–ƒâ–„â–„â–ƒâ–„â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒ</td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>charts/SPS</td><td>1203</td></tr><tr><td>charts/episodic_length</td><td>98</td></tr><tr><td>charts/episodic_return</td><td>98</td></tr><tr><td>charts/learning_rate</td><td>1e-05</td></tr><tr><td>global_step</td><td>24576</td></tr><tr><td>losses/approx_kl</td><td>0.0</td></tr><tr><td>losses/clipfrac</td><td>0</td></tr><tr><td>losses/entropy</td><td>0.61023</td></tr><tr><td>losses/explained_variance</td><td>0.08197</td></tr><tr><td>losses/old_approx_kl</td><td>0.0</td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">CartPole-v1__ppo_experiment__1__1767293184</strong> at: <a href='https://wandb.ai/mishra39/ppo-implementation-details/runs/rvk007tg' target=\"_blank\">https://wandb.ai/mishra39/ppo-implementation-details/runs/rvk007tg</a><br> View project at: <a href='https://wandb.ai/mishra39/ppo-implementation-details' target=\"_blank\">https://wandb.ai/mishra39/ppo-implementation-details</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20260101_184624-rvk007tg/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260101_191909-lwpqyud6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mishra39/ppo-implementation-details/runs/lwpqyud6' target=\"_blank\">CartPole-v1__ppo_experiment__1__1767295149</a></strong> to <a href='https://wandb.ai/mishra39/ppo-implementation-details' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mishra39/ppo-implementation-details' target=\"_blank\">https://wandb.ai/mishra39/ppo-implementation-details</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mishra39/ppo-implementation-details/runs/lwpqyud6' target=\"_blank\">https://wandb.ai/mishra39/ppo-implementation-details/runs/lwpqyud6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# env setup\n",
        "# envs = gym.vector.SyncVectorEnv(\n",
        "#     [make_env(args.gym_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
        "# )\n",
        "# obs, infos = envs.reset()\n",
        "run_name = f\"{config.env_id}__ppo__{int(time.time())}\"\n",
        "os.makedirs(config.video_dir, exist_ok=True)\n",
        "\n",
        "envs = gym.vector.SyncVectorEnv(\n",
        "    [make_env(config.env_id, seed=args.seed + i, idx=i, capture_video=args.capture_video, run_name=run_name)\n",
        "     for i in range(config.num_envs)]\n",
        ")\n",
        "obs, infos = envs.reset()"
      ],
      "metadata": {
        "id": "yIXXhylVD-5C"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "def log_latest_training_video(step: int):\n",
        "    video_glob = os.path.join(config.video_dir, run_name, \"*.mp4\")\n",
        "    video_files = sorted(glob.glob(video_glob), key=os.path.getmtime)\n",
        "    if not video_files:\n",
        "        return\n",
        "    latest_video = video_files[-1]\n",
        "    wandb.log({\"train/video\": wandb.Video(latest_video, caption=f\"Step {step}\")}, step=step)"
      ],
      "metadata": {
        "id": "12DEsYdMAiSe"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Setup"
      ],
      "metadata": {
        "id": "r1j3xFIkigxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent(nn.Module):\n",
        "  def __init__(self, envs):\n",
        "    super(Agent, self).__init__()\n",
        "\n",
        "    '''\n",
        "    - Estimates the Value Function $V(s)$. This is a scalar prediction of the total expected reward an agent will receive starting from state s.\n",
        "\n",
        "    - Tanh is often preferred in PPO (and standard implementations like CleanRL) because it produces smoother gradients. Since the Critic is trying to map states to a continuous value, a smooth activation function helps the Advantage calculation stay stable.\n",
        "\n",
        "    - Notice std=1. In PPO, initializing the last layer of the critic with a standard deviation of 1 is a common practice to ensure the initial value estimates aren't near zero, helping the policy gradients have a meaningful \"baseline\" to compare against immediately\n",
        "\n",
        "    - In PPO, the Critic's job is to reduce variance'''\n",
        "\n",
        "    self.critic = nn.Sequential(\n",
        "        layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
        "        nn.Tanh(),\n",
        "        layer_init(nn.Linear(64,64)),\n",
        "        nn.Tanh(),\n",
        "        layer_init(nn.Linear(64,1), std=1),\n",
        "    )\n",
        "\n",
        "    self.actor = nn.Sequential(\n",
        "        layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
        "        nn.Tanh(),\n",
        "        layer_init(nn.Linear(64,64)),\n",
        "        nn.Tanh(),\n",
        "        layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01), # smaller std dev ensures similar values for all actions -> probability is similar for picking each action at the beginning\n",
        "    )\n",
        "\n",
        "  def get_value(self, x):\n",
        "    return self.critic(x)\n",
        "\n",
        "  def get_action_and_value(self, x, action=None):\n",
        "    logits = self.actor(x)\n",
        "    probs = Categorical(logits=logits)\n",
        "    if action is None:\n",
        "      action = probs.sample()\n",
        "    return action, probs.log_prob(action), probs.entropy(), self.critic(x)"
      ],
      "metadata": {
        "id": "6GkzIuuPaAdQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "qVOkLvNJiPgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(envs).to(device)\n",
        "optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
        "\n",
        "# ALGO Logic: Storage setup\n",
        "obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
        "actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
        "\n",
        "# Size: N (num_envs) * T (timesteps)\n",
        "logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "rewards = torch.zeros_like(logprobs).to(device)\n",
        "dones = torch.zeros_like(logprobs).to(device)\n",
        "values = torch.zeros_like(logprobs).to(device)\n",
        "\n",
        "# TRY NOT TO MODIFY: start the game\n",
        "global_step = 0\n",
        "next_obs, _ = envs.reset()\n",
        "next_obs = torch.Tensor(next_obs).to(device)\n",
        "next_done = torch.zeros(args.num_envs).to(device)\n",
        "num_updates = args.total_timesteps // args.batch_size\n",
        "print(f\"total_timesteps: {args.total_timesteps}\")\n",
        "print(f\"batch_size: {args.batch_size}\")\n",
        "print(f\"num_updates: {num_updates}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRXid6qviR1Z",
        "outputId": "3ffb3f1c-c4dd-4c74-8523-859096f264ef"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_timesteps: 25000\n",
            "batch_size: 512\n",
            "num_updates: 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "VilCWB6zllWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "for update in range(1, num_updates + 1): # ALGO 1: Line #1\n",
        "  # lr annealing\n",
        "  if args.anneal_lr:\n",
        "    frac = 1.0 - (update - 1.0) / num_updates\n",
        "    lrnow = frac * args.learning_rate\n",
        "    optimizer.param_groups[0][\"lr\"] = lrnow\n",
        "    writer.add_scalar(\"charts/learning_rate\", lrnow, global_step)\n",
        "    if args.track:\n",
        "            wandb.log({\n",
        "                \"charts/learning_rate\": lrnow,\n",
        "            }, step=global_step)\n",
        "\n",
        "            if args.capture_video and (update % 10 == 0):\n",
        "              log_latest_training_video(global_step)\n",
        "\n",
        "\n",
        "  # policy rollout\n",
        "  for step in range(0, args.num_steps): # ALGO1: Line #3\n",
        "    global_step += 1 * args.num_envs\n",
        "    obs[step] = next_obs\n",
        "    dones[step] = next_done\n",
        "\n",
        "    # Algo Logic: Action logic\n",
        "    with torch.no_grad():\n",
        "      action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
        "      values[step] = value.squeeze(-1) # dimension: (4,1)\n",
        "    actions[step] = action # dim: (4, action_size)\n",
        "    logprobs[step] = log_prob\n",
        "\n",
        "    # TRY NOT TO MODIFY: execute the game and log data.\n",
        "    next_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
        "    done = np.logical_or(terminated, truncated)  # Combine terminated and truncated into done\n",
        "\n",
        "    rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
        "    next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
        "\n",
        "    # Log episodic returns when episodes finish\n",
        "    if \"episode\" in info and info[\"_episode\"].any():\n",
        "      finished_indices = np.where(info[\"_episode\"])[0]\n",
        "\n",
        "      for idx in finished_indices:\n",
        "        episodic_return = info[\"episode\"][\"r\"][idx]\n",
        "        episodic_length = info[\"episode\"][\"l\"][idx]\n",
        "\n",
        "        print(f\"global_step={global_step}, episodic_return={episodic_return}\")\n",
        "        writer.add_scalar(\"charts/episodic_return\", episodic_return, global_step)\n",
        "        writer.add_scalar(\"charts/episodic_length\", episodic_length, global_step)\n",
        "\n",
        "        # Optionally log to wandb\n",
        "        if args.track:\n",
        "            wandb.log({\n",
        "                \"charts/episodic_return\": episodic_return,\n",
        "                \"charts/episodic_length\": episodic_length,\n",
        "            }, step=global_step)\n",
        "        break  # Log only the first finished episode per step\n",
        "\n",
        "  # bootstrap value if not done\n",
        "  with torch.no_grad():\n",
        "    next_value = agent.get_value(next_obs).reshape(1,-1)\n",
        "    if args.gae:\n",
        "      advantages = torch.zeros_like(rewards).to(device) # dim: (num_env, num_timesteps) = (N,T)\n",
        "      lastgaelam = 0\n",
        "      for t in reversed(range(args.num_steps)): # ALGO1: Line #4\n",
        "          if t == args.num_steps - 1:\n",
        "              nextnonterminal = 1.0 - next_done\n",
        "              nextvalues = next_value\n",
        "          else:\n",
        "              nextnonterminal = 1.0 - dones[t + 1]\n",
        "              nextvalues = values[t + 1]\n",
        "          delta = rewards[t] + nextvalues * nextnonterminal * args.gamma - values[t] # Eq. 12 (Temporal Difference Error)\n",
        "          advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam # Eq. 11 (Recursive Form)\n",
        "      returns = advantages + values\n",
        "    else:\n",
        "      returns = torch.zeros_like(rewards).to(device)\n",
        "      for t in reversed(range(args.num_steps)):\n",
        "          if t == args.num_steps - 1:\n",
        "              nextnonterminal = 1.0 - next_done\n",
        "              next_return = next_value\n",
        "          else:\n",
        "              nextnonterminal = 1.0 - dones[t + 1]\n",
        "              next_return = returns[t + 1]\n",
        "          returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return\n",
        "      advantages = returns - values\n",
        "  # flatten the batch\n",
        "  b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
        "  b_logprobs = logprobs.reshape(-1)\n",
        "  b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
        "  b_advantages = advantages.reshape(-1)\n",
        "  b_returns = returns.reshape(-1)\n",
        "  b_values = values.reshape(-1)\n",
        "\n",
        "  # Optimizing the policy and value network\n",
        "  b_inds = np.arange(args.batch_size) # 512\n",
        "  clipfracs = []\n",
        "  for epoch in range(args.update_epochs):\n",
        "    np.random.shuffle(b_inds)\n",
        "    for start in range(0, args.batch_size, args.minibatch_size):\n",
        "      end = start + args.minibatch_size\n",
        "      mb_inds = b_inds[start:end]\n",
        "      _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
        "      logratio = newlogprob - b_logprobs[mb_inds]\n",
        "      ratio = logratio.exp()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
        "        old_approx_kl = (-logratio).mean()\n",
        "        approx_kl = ((ratio - 1) - logratio).mean()\n",
        "        clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
        "\n",
        "      mb_advantages = b_advantages[mb_inds]\n",
        "      if args.norm_adv:\n",
        "        mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
        "\n",
        "      # Policy loss\n",
        "      pg_loss1 = -mb_advantages * ratio # Eq.6 (L_CPI)\n",
        "      pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef) # Eq.7 (L_CLIP)\n",
        "      pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
        "\n",
        "      # Value loss\n",
        "      newvalue = newvalue.view(-1)\n",
        "      if args.clip_vloss:\n",
        "        v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
        "        v_clipped = b_values[mb_inds] + torch.clamp(\n",
        "                        newvalue - b_values[mb_inds],\n",
        "                        -args.clip_coef,\n",
        "                        args.clip_coef,\n",
        "                    )\n",
        "        v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
        "        v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
        "        v_loss = 0.5 * v_loss_max.mean()\n",
        "      else:\n",
        "        v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
        "\n",
        "      # Entropy loss\n",
        "      entropy_loss = entropy.mean()\n",
        "      # Overall loss\n",
        "      loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
        "\n",
        "      # Backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      # Clip gradient\n",
        "      nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
        "      optimizer.step()\n",
        "\n",
        "    if args.target_kl is not None:\n",
        "      if approx_kl > args.target_kl:\n",
        "        break\n",
        "  y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
        "  var_y = np.var(y_true)\n",
        "  explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
        "\n",
        "  # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
        "  writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
        "  writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
        "  writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
        "  writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
        "  writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
        "  writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
        "  writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
        "  writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
        "  print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
        "  writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
        "\n",
        "envs.close()\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSZu-9RClnVV",
        "outputId": "769fdac2-62b2-4e17-837c-094ae168aee9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "global_step=24592, episodic_return=109.0\n",
            "global_step=24760, episodic_return=97.0\n",
            "global_step=24952, episodic_return=47.0\n",
            "global_step=24964, episodic_return=120.0\n",
            "global_step=25072, episodic_return=119.0\n",
            "SPS: 53630\n",
            "global_step=25216, episodic_return=62.0\n",
            "global_step=25248, episodic_return=43.0\n",
            "global_step=25284, episodic_return=82.0\n",
            "global_step=25436, episodic_return=120.0\n",
            "global_step=25472, episodic_return=55.0\n",
            "SPS: 14458\n",
            "global_step=25736, episodic_return=129.0\n",
            "global_step=25788, episodic_return=87.0\n",
            "global_step=25812, episodic_return=84.0\n",
            "global_step=25884, episodic_return=149.0\n",
            "global_step=25952, episodic_return=53.0\n",
            "global_step=26064, episodic_return=68.0\n",
            "global_step=26088, episodic_return=68.0\n",
            "SPS: 6348\n",
            "global_step=26160, episodic_return=51.0\n",
            "global_step=26268, episodic_return=95.0\n",
            "global_step=26288, episodic_return=31.0\n",
            "global_step=26592, episodic_return=125.0\n",
            "SPS: 4550\n",
            "global_step=26628, episodic_return=140.0\n",
            "global_step=26756, episodic_return=116.0\n",
            "global_step=26796, episodic_return=131.0\n",
            "global_step=27056, episodic_return=106.0\n",
            "SPS: 3719\n",
            "global_step=27164, episodic_return=91.0\n",
            "global_step=27484, episodic_return=106.0\n",
            "global_step=27648, episodic_return=222.0\n",
            "SPS: 3483\n",
            "global_step=27656, episodic_return=265.0\n",
            "global_step=27692, episodic_return=131.0\n",
            "global_step=28048, episodic_return=99.0\n",
            "SPS: 2458\n",
            "global_step=28308, episodic_return=153.0\n",
            "global_step=28476, episodic_return=247.0\n",
            "SPS: 2370\n",
            "global_step=28820, episodic_return=192.0\n",
            "global_step=28860, episodic_return=300.0\n",
            "global_step=29040, episodic_return=182.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SPS: 2120\n",
            "global_step=29464, episodic_return=246.0\n",
            "global_step=29608, episodic_return=141.0\n",
            "SPS: 2006\n",
            "global_step=30004, episodic_return=285.0\n",
            "SPS: 1886\n",
            "global_step=30328, episodic_return=215.0\n",
            "global_step=30420, episodic_return=399.0\n",
            "global_step=30488, episodic_return=120.0\n",
            "SPS: 1453\n",
            "global_step=30820, episodic_return=302.0\n",
            "global_step=30872, episodic_return=112.0\n",
            "SPS: 1355\n",
            "global_step=31424, episodic_return=233.0\n",
            "global_step=31716, episodic_return=72.0\n",
            "SPS: 1322\n",
            "global_step=31748, episodic_return=354.0\n",
            "global_step=31764, episodic_return=222.0\n",
            "global_step=31852, episodic_return=257.0\n",
            "global_step=32092, episodic_return=93.0\n",
            "SPS: 1242\n",
            "global_step=32536, episodic_return=192.0\n",
            "global_step=32608, episodic_return=214.0\n",
            "global_step=32624, episodic_return=192.0\n",
            "SPS: 1185\n",
            "global_step=32836, episodic_return=185.0\n",
            "global_step=33088, episodic_return=137.0\n",
            "SPS: 1139\n",
            "global_step=33404, episodic_return=141.0\n",
            "SPS: 1132\n",
            "global_step=33868, episodic_return=194.0\n",
            "global_step=33976, episodic_return=142.0\n",
            "global_step=34128, episodic_return=379.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SPS: 1088\n",
            "global_step=34616, episodic_return=159.0\n",
            "global_step=34728, episodic_return=214.0\n",
            "SPS: 1062\n",
            "global_step=34944, episodic_return=203.0\n",
            "SPS: 1038\n",
            "global_step=35332, episodic_return=365.0\n",
            "SPS: 925\n",
            "global_step=36068, episodic_return=362.0\n",
            "global_step=36076, episodic_return=185.0\n",
            "SPS: 878\n",
            "global_step=36376, episodic_return=411.0\n",
            "global_step=36500, episodic_return=388.0\n",
            "global_step=36564, episodic_return=123.0\n",
            "global_step=36780, episodic_return=175.0\n",
            "SPS: 839\n",
            "global_step=37308, episodic_return=232.0\n",
            "SPS: 835\n",
            "global_step=37768, episodic_return=300.0\n",
            "global_step=37808, episodic_return=256.0\n",
            "SPS: 812\n",
            "SPS: 812\n",
            "global_step=38420, episodic_return=162.0\n",
            "global_step=38504, episodic_return=500.0\n",
            "global_step=38696, episodic_return=221.0\n",
            "global_step=38812, episodic_return=375.0\n",
            "SPS: 788\n",
            "global_step=39276, episodic_return=213.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SPS: 782\n",
            "global_step=39492, episodic_return=198.0\n",
            "global_step=39712, episodic_return=301.0\n",
            "SPS: 762\n",
            "global_step=40192, episodic_return=174.0\n",
            "global_step=40212, episodic_return=124.0\n",
            "SPS: 748\n",
            "global_step=40452, episodic_return=293.0\n",
            "global_step=40816, episodic_return=500.0\n",
            "SPS: 749\n",
            "global_step=40964, episodic_return=192.0\n",
            "global_step=41160, episodic_return=236.0\n",
            "global_step=41300, episodic_return=211.0\n",
            "SPS: 736\n",
            "global_step=41516, episodic_return=137.0\n",
            "global_step=41568, episodic_return=187.0\n",
            "SPS: 718\n",
            "global_step=42056, episodic_return=188.0\n",
            "global_step=42100, episodic_return=132.0\n",
            "global_step=42308, episodic_return=197.0\n",
            "global_step=42344, episodic_return=295.0\n",
            "SPS: 706\n",
            "global_step=42728, episodic_return=167.0\n",
            "SPS: 707\n",
            "global_step=43060, episodic_return=187.0\n",
            "global_step=43096, episodic_return=248.0\n",
            "global_step=43344, episodic_return=249.0\n",
            "SPS: 691\n",
            "global_step=43780, episodic_return=170.0\n",
            "global_step=43900, episodic_return=209.0\n",
            "SPS: 673\n",
            "global_step=44076, episodic_return=336.0\n",
            "global_step=44424, episodic_return=269.0\n",
            "global_step=44508, episodic_return=181.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SPS: 674\n",
            "global_step=44764, episodic_return=215.0\n",
            "global_step=44808, episodic_return=182.0\n",
            "global_step=44920, episodic_return=123.0\n",
            "SPS: 664\n",
            "global_step=45244, episodic_return=119.0\n",
            "global_step=45476, episodic_return=138.0\n",
            "global_step=45552, episodic_return=260.0\n",
            "global_step=45564, episodic_return=188.0\n",
            "SPS: 657\n",
            "global_step=45860, episodic_return=153.0\n",
            "SPS: 650\n",
            "global_step=46176, episodic_return=155.0\n",
            "global_step=46320, episodic_return=188.0\n",
            "global_step=46376, episodic_return=224.0\n",
            "SPS: 651\n",
            "global_step=46716, episodic_return=134.0\n",
            "global_step=46976, episodic_return=278.0\n",
            "SPS: 641\n",
            "global_step=47124, episodic_return=186.0\n",
            "global_step=47184, episodic_return=215.0\n",
            "global_step=47344, episodic_return=156.0\n",
            "SPS: 642\n",
            "global_step=48116, episodic_return=192.0\n",
            "SPS: 636\n",
            "global_step=48168, episodic_return=297.0\n",
            "global_step=48368, episodic_return=310.0\n",
            "SPS: 619\n",
            "global_step=48768, episodic_return=149.0\n",
            "global_step=48980, episodic_return=448.0\n",
            "global_step=49076, episodic_return=239.0\n",
            "global_step=49088, episodic_return=179.0\n",
            "SPS: 613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "FMotouiS6nn3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_eval_env():\n",
        "    env = gym.make(config.env_id, render_mode=\"rgb_array\")\n",
        "    eval_folder = config.video_eval_dir\n",
        "    os.makedirs(eval_folder, exist_ok=True)\n",
        "    env = gym.wrappers.RecordVideo(\n",
        "        env,\n",
        "        video_folder=eval_folder,\n",
        "        name_prefix=\"eval\",\n",
        "        episode_trigger=lambda episode_id: True,\n",
        "        disable_logger=True,\n",
        "    )\n",
        "    return env\n",
        "\n",
        "def evaluate_and_log_videos(model: Agent, episodes: int = None):\n",
        "    if episodes is None:\n",
        "        episodes = config.eval_episodes\n",
        "\n",
        "    env = make_eval_env()\n",
        "    for ep in range(episodes):\n",
        "        obs, info = env.reset()\n",
        "        done = False\n",
        "        truncated = False\n",
        "        total_r = 0.0\n",
        "\n",
        "        while not (done or truncated):\n",
        "            obs_t = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                action, _, _, _ = model.get_action_and_value(obs_t)\n",
        "            obs, reward, done, truncated, info = env.step(action.cpu().numpy()[0])\n",
        "            total_r += reward\n",
        "\n",
        "        wandb.log({\"eval/return\": total_r}, step=global_step)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # upload all eval videos\n",
        "    video_files = sorted(glob.glob(os.path.join(config.video_eval_dir, \"*.mp4\")), key=os.path.getmtime)\n",
        "    for vf in video_files[-episodes:]:\n",
        "        wandb.log({\"eval/video\": wandb.Video(vf)}, step=global_step)"
      ],
      "metadata": {
        "id": "YuhGBoxL6jKn"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the evaluation\n",
        "evaluate_and_log_videos(agent, episodes=config.eval_episodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YqGgUjI6xux",
        "outputId": "174fda46-37fb-4ac4-dba8-ac561d0ed530"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gymnasium/wrappers/rendering.py:293: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos_eval folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `format` argument was not provided, defaulting to `gif`. This parameter will be required in v0.20.0, please specify the format explicitly.\n"
          ]
        }
      ]
    }
  ]
}