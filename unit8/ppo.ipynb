{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMbrWaSH33gpMBpQ0Es5W1Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mishra39/deep_rl_course_hf/blob/main/unit8/ppo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a virtual display ğŸ”½\n",
        "\n",
        "During the notebook, we'll need to generate a replay video. To do so, with colab, **we need to have a virtual screen to be able to render the environment** (and thus record the frames).\n",
        "\n",
        "Hence the following cell will install the librairies and create and run a virtual screen ğŸ–¥"
      ],
      "metadata": {
        "id": "bTpYcVZVMzUI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jV6wjQ7Be7p5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay\n",
        "!apt install x11-utils\n",
        "!pip install pyglet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "id": "ww5PQH1gNLI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "665cfcba-81b1-48bd-c82d-9bd68aed2c26"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7cb8f39bf9e0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display video\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "def show_video():\n",
        "    \"\"\"Embeds the recorded video in the notebook output.\"\"\"\n",
        "    mp4list = glob.glob('videos/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        video = mp4list[0]\n",
        "        with io.open(video, 'r+b') as f:\n",
        "            encoded = base64.b64encode(f.read()).decode()\n",
        "        # Create an HTML display object for Colab\n",
        "        return HTML(data=f'<video width=\"1000\" controls><source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\" /></video>')\n",
        "    else:\n",
        "        print(\"No video files found in the 'videos' directory.\")"
      ],
      "metadata": {
        "id": "7js6sBTwmEg_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install dependencies ğŸ”½\n",
        "\n",
        "The first step is to install the dependencies, weâ€™ll install multiple ones:\n",
        "- `gymnasium`\n",
        "- `panda-gym`: Contains the robotics arm environments.\n",
        "- `stable-baselines3`: The SB3 deep reinforcement learning library.\n",
        "- `huggingface_sb3`: Additional code for Stable-baselines3 to load and upload models from the Hugging Face ğŸ¤— Hub.\n",
        "- `huggingface_hub`: Library allowing anyone to work with the Hub repositories.\n",
        "\n",
        "â² The installation can **take 10 minutes**."
      ],
      "metadata": {
        "id": "e1obkbdJ_KnG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install stable-baselines3[extra]\n",
        "!pip install gymnasium"
      ],
      "metadata": {
        "id": "TgZUkjKYSgvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73a60c9f-d8e9-4b52-8d5a-73842276633a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install huggingface_sb3\n",
        "# !pip install huggingface_hub\n",
        "# !pip install panda_gym"
      ],
      "metadata": {
        "id": "ABneW6tOSpyU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6BLqKndoR1t"
      },
      "source": [
        "## W&B Prerequisites\n",
        "\n",
        "Install the W&B Python SDK and log in:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ImmYONLQoR1u"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -qU\n",
        "!pip install -q gym numpy tensorboard\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xjomxlWXoR1w"
      },
      "outputs": [],
      "source": [
        "# Log in to your W&B account\n",
        "import wandb\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_cQ5-08BoR1x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64ead200-0005-46c7-cfe9-0b971c449dc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into https://api.wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find your API key here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmishra39\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "h-cj5rkzpwGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from distutils.util import strtobool\n",
        "\n",
        "import gymnasium as gym  # Use gymnasium instead of gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.distributions.categorical import Categorical\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "metadata": {
        "id": "W2bqN8jzp564"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arguments"
      ],
      "metadata": {
        "id": "GkxawpDv85Nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import Optional\n",
        "\n",
        "@dataclass\n",
        "class PPOConfig:\n",
        "    # Experiment settings\n",
        "    exp_name: str = \"ppo_experiment\"\n",
        "    gym_id: str = \"CartPole-v1\"\n",
        "    learning_rate: float = 2.5e-4\n",
        "    seed: int = 1\n",
        "    total_timesteps: int = 25000\n",
        "    torch_deterministic: bool = True\n",
        "    cuda: bool = True\n",
        "    track: bool = False\n",
        "    wandb_project_name: str = \"ppo-implementation-details\"\n",
        "    wandb_entity: Optional[str] = None\n",
        "    capture_video: bool = False\n",
        "\n",
        "    # Algorithm specific arguments\n",
        "    num_envs: int = 4\n",
        "    num_steps: int = 128\n",
        "    anneal_lr: bool = True\n",
        "    gae: bool = True\n",
        "    gamma: float = 0.99\n",
        "    gae_lambda: float = 0.95\n",
        "    num_minibatches: int = 4\n",
        "    update_epochs: int = 4\n",
        "    norm_adv: bool = True\n",
        "    clip_coef: float = 0.2\n",
        "    clip_vloss: bool = True\n",
        "    ent_coef: float = 0.01\n",
        "    vf_coef: float = 0.5\n",
        "    max_grad_norm: float = 0.5\n",
        "    target_kl: Optional[float] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        # Computed values\n",
        "        self.batch_size = int(self.num_envs * self.num_steps)\n",
        "        self.minibatch_size = int(self.batch_size // self.num_minibatches)\n",
        "\n",
        "# Create instance with default values\n",
        "# args = PPOConfig()\n",
        "\n",
        "# Or customize specific values\n",
        "# args = PPOConfig(learning_rate=1e-3, num_envs=8, total_timesteps=50000)\n",
        "\n",
        "# print(f\"Batch size: {args.batch_size}\")\n",
        "# print(f\"Minibatch size: {args.minibatch_size}\")"
      ],
      "metadata": {
        "id": "6hwqBmy387R9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gym Envrionment"
      ],
      "metadata": {
        "id": "HTO4Ho3z9EBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env(gym_id, seed, idx, capture_video, run_name):\n",
        "    def thunk():\n",
        "      env = gym.make(gym_id, render_mode=\"rgb_array\" if capture_video else None)\n",
        "      env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "      if capture_video:\n",
        "            if idx == 0:\n",
        "              env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
        "              # disable_logger=True) # added to avoid conflict with wandb logger\n",
        "      env.action_space.seed(seed)\n",
        "      env.observation_space.seed(seed)\n",
        "      return env\n",
        "\n",
        "    return thunk"
      ],
      "metadata": {
        "id": "GclrE21F9MZ9"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layer Initialization"
      ],
      "metadata": {
        "id": "PqwnPqULZ9Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
        "  torch.nn.init.orthogonal_(layer.weight, std)\n",
        "  torch.nn.init.constant_(layer.bias, bias_const)\n",
        "  return layer"
      ],
      "metadata": {
        "id": "KN_khmRUbdG1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Loop"
      ],
      "metadata": {
        "id": "C5nZUu1b92zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = PPOConfig(track=True, capture_video=True) # Changed capture_video to True\n",
        "run_name = f\"{args.gym_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
        "if args.track:\n",
        "    wandb.init(\n",
        "        project=args.wandb_project_name,\n",
        "        entity=args.wandb_entity,\n",
        "        sync_tensorboard=True,\n",
        "        config=vars(args),\n",
        "        name=run_name,\n",
        "        monitor_gym=False, # Set to False to prevent conflict with RecordVideo\n",
        "        save_code=True,\n",
        "    )\n",
        "writer = SummaryWriter(f\"runs/{run_name}\")\n",
        "writer.add_text(\n",
        "    \"hyperparameters\",\n",
        "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
        ")\n",
        "\n",
        "# TRY NOT TO MODIFY: seeding\n",
        "random.seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "id": "iLgY25BJ94qq",
        "outputId": "71ba8f93-5996-4308-ea2c-01f245c7a280"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>charts/SPS</td><td>â–…â–…â–†â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–‡â–†â–…â–„â–‚â–â–â–â–‚â–ƒâ–ƒâ–ƒâ–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡</td></tr><tr><td>charts/episodic_length</td><td>â–‚â–â–‚â–â–â–‚â–â–â–â–â–â–‚â–â–‚â–‚â–„â–‚â–ƒâ–‚â–‚â–‚â–ˆâ–â–‚â–â–ƒâ–‚â–â–†â–‚â–ƒâ–ƒâ–ƒâ–‚â–…â–ƒâ–‚â–„â–ƒâ–ƒ</td></tr><tr><td>charts/episodic_return</td><td>â–â–‚â–â–â–‚â–â–‚â–â–‚â–‚â–ƒâ–â–‚â–‚â–‚â–‚â–‚â–„â–‚â–â–â–‚â–â–â–„â–†â–‚â–ˆâ–â–ƒâ–ƒâ–â–ƒâ–…â–ƒâ–‚â–ƒâ–„â–…â–‚</td></tr><tr><td>charts/learning_rate</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–†â–†â–†â–…â–…â–…â–…â–…â–‡â–†â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–…â–…â–…â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–</td></tr><tr><td>global_step</td><td>â–â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–ˆ</td></tr><tr><td>losses/approx_kl</td><td>â–…â–„â–ƒâ–‚â–…â–ˆâ–„â–„â–„â–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–â–â–ƒâ–ƒâ–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>losses/clipfrac</td><td>â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–</td></tr><tr><td>losses/entropy</td><td>â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–†â–‡â–†â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–„â–„â–„â–ƒâ–‚â–‚â–â–ƒâ–ƒâ–ƒâ–ƒâ–â–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–â–‚â–‚â–ƒ</td></tr><tr><td>losses/explained_variance</td><td>â–‚â–ƒâ–‚â–ƒâ–‚â–â–‚â–‚â–ƒâ–ƒâ–‚â–‚â–â–‚â–â–â–ƒâ–‚â–‚â–ƒâ–„â–‚â–ƒâ–„â–ƒâ–‡â–…â–ƒâ–ˆâ–ˆâ–†â–…â–„â–‚â–†â–‚â–ƒâ–†â–„â–„</td></tr><tr><td>losses/old_approx_kl</td><td>â–„â–ƒâ–ƒâ–â–ƒâ–…â–†â–…â–„â–ˆâ–ƒâ–†â–â–„â–„â–„â–ƒâ–„â–…â–…â–ƒâ–„â–„â–…â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„â–„</td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>charts/SPS</td><td>1564</td></tr><tr><td>charts/episodic_length</td><td>129</td></tr><tr><td>charts/episodic_return</td><td>129</td></tr><tr><td>charts/learning_rate</td><td>1e-05</td></tr><tr><td>global_step</td><td>24576</td></tr><tr><td>losses/approx_kl</td><td>0.0</td></tr><tr><td>losses/clipfrac</td><td>0</td></tr><tr><td>losses/entropy</td><td>0.61483</td></tr><tr><td>losses/explained_variance</td><td>0.03128</td></tr><tr><td>losses/old_approx_kl</td><td>-1e-05</td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">CartPole-v1__ppo_experiment__1__1767100305</strong> at: <a href='https://wandb.ai/mishra39/ppo-implementation-details/runs/zz4do7a7' target=\"_blank\">https://wandb.ai/mishra39/ppo-implementation-details/runs/zz4do7a7</a><br> View project at: <a href='https://wandb.ai/mishra39/ppo-implementation-details' target=\"_blank\">https://wandb.ai/mishra39/ppo-implementation-details</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251230_131145-zz4do7a7/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251230_131338-2qk4gakj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mishra39/ppo-implementation-details/runs/2qk4gakj' target=\"_blank\">CartPole-v1__ppo_experiment__1__1767100417</a></strong> to <a href='https://wandb.ai/mishra39/ppo-implementation-details' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mishra39/ppo-implementation-details' target=\"_blank\">https://wandb.ai/mishra39/ppo-implementation-details</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mishra39/ppo-implementation-details/runs/2qk4gakj' target=\"_blank\">https://wandb.ai/mishra39/ppo-implementation-details/runs/2qk4gakj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m When using several event log directories, please call `wandb.tensorboard.patch(root_logdir=\"...\")` before `wandb.init`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# env setup\n",
        "envs = gym.vector.SyncVectorEnv(\n",
        "    [make_env(args.gym_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
        ")\n",
        "obs, infos = envs.reset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIXXhylVD-5C",
        "outputId": "0d5a91d8-e2ef-4118-9fe9-a950d595220e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/moviepy/config_defaults.py:47: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  IMAGEMAGICK_BINARY = r\"C:\\Program Files\\ImageMagick-6.8.8-Q16\\magick.exe\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agent Setup"
      ],
      "metadata": {
        "id": "r1j3xFIkigxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent(nn.Module):\n",
        "  def __init__(self, envs):\n",
        "    super(Agent, self).__init__()\n",
        "\n",
        "    '''\n",
        "    - Estimates the Value Function $V(s)$. This is a scalar prediction of the total expected reward an agent will receive starting from state s.\n",
        "\n",
        "    - Tanh is often preferred in PPO (and standard implementations like CleanRL) because it produces smoother gradients. Since the Critic is trying to map states to a continuous value, a smooth activation function helps the Advantage calculation stay stable.\n",
        "\n",
        "    - Notice std=1. In PPO, initializing the last layer of the critic with a standard deviation of 1 is a common practice to ensure the initial value estimates aren't near zero, helping the policy gradients have a meaningful \"baseline\" to compare against immediately\n",
        "\n",
        "    - In PPO, the Critic's job is to reduce variance'''\n",
        "\n",
        "    self.critic = nn.Sequential(\n",
        "        layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
        "        nn.Tanh(),\n",
        "        layer_init(nn.Linear(64,64)),\n",
        "        nn.Tanh(),\n",
        "        layer_init(nn.Linear(64,1), std=1),\n",
        "    )\n",
        "\n",
        "    self.actor = nn.Sequential(\n",
        "        layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
        "        nn.Tanh(),\n",
        "        layer_init(nn.Linear(64,64)),\n",
        "        nn.Tanh(),\n",
        "        layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01), # smaller std dev ensures similar values for all actions -> probability is similar for picking each action at the beginning\n",
        "    )\n",
        "\n",
        "  def get_value(self, x):\n",
        "    return self.critic(x)\n",
        "\n",
        "  def get_action_and_value(self, x, action=None):\n",
        "    logits = self.actor(x)\n",
        "    probs = Categorical(logits=logits)\n",
        "    if action is None:\n",
        "      action = probs.sample()\n",
        "    return action, probs.log_prob(action), probs.entropy(), self.critic(x)"
      ],
      "metadata": {
        "id": "6GkzIuuPaAdQ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "qVOkLvNJiPgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(envs).to(device)\n",
        "optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
        "\n",
        "# ALGO Logic: Storage setup\n",
        "obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
        "actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
        "logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
        "rewards = torch.zeros_like(logprobs).to(device)\n",
        "dones = torch.zeros_like(logprobs).to(device)\n",
        "values = torch.zeros_like(logprobs).to(device)\n",
        "\n",
        "# TRY NOT TO MODIFY: start the game\n",
        "global_step = 0\n",
        "next_obs, _ = envs.reset()\n",
        "next_obs = torch.Tensor(next_obs).to(device)\n",
        "next_done = torch.zeros(args.num_envs).to(device)\n",
        "num_updates = args.total_timesteps // args.batch_size\n",
        "print(f\"total_timesteps: {args.total_timesteps}\")\n",
        "print(f\"batch_size: {args.batch_size}\")\n",
        "print(f\"num_updates: {num_updates}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRXid6qviR1Z",
        "outputId": "4f1b0a32-d1a1-4d18-e808-31112318a2a6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_timesteps: 25000\n",
            "batch_size: 512\n",
            "num_updates: 48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "VilCWB6zllWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "for update in range(1, num_updates + 1):\n",
        "  # lr annealing\n",
        "  if args.anneal_lr:\n",
        "    frac = 1.0 - (update - 1.0) / num_updates\n",
        "    lrnow = frac * args.learning_rate\n",
        "    optimizer.param_groups[0][\"lr\"] = lrnow\n",
        "    writer.add_scalar(\"charts/learning_rate\", lrnow, global_step)\n",
        "    if args.track:\n",
        "            wandb.log({\n",
        "                \"charts/learning_rate\": lrnow,\n",
        "            }, step=global_step)\n",
        "\n",
        "  # policy rollout\n",
        "  for step in range(0, args.num_steps):\n",
        "    global_step += 1 * args.num_envs\n",
        "    obs[step] = next_obs\n",
        "    dones[step] = next_done\n",
        "\n",
        "    # Algo Logic: Action logic\n",
        "    with torch.no_grad():\n",
        "      action, log_prob, _, value = agent.get_action_and_value(next_obs)\n",
        "      values[step] = value.squeeze(-1) # Corrected line: squeeze the last dimension\n",
        "    actions[step] = action\n",
        "    logprobs[step] = log_prob\n",
        "\n",
        "    # TRY NOT TO MODIFY: execute the game and log data.\n",
        "    next_obs, reward, terminated, truncated, info = envs.step(action.cpu().numpy())\n",
        "    done = np.logical_or(terminated, truncated)  # Combine terminated and truncated into done\n",
        "\n",
        "    rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
        "    next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
        "\n",
        "    # Log episodic returns when episodes finish\n",
        "    if \"episode\" in info and info[\"_episode\"].any():\n",
        "      finished_indices = np.where(info[\"_episode\"])[0]\n",
        "\n",
        "      for idx in finished_indices:\n",
        "        episodic_return = info[\"episode\"][\"r\"][idx]\n",
        "        episodic_length = info[\"episode\"][\"l\"][idx]\n",
        "\n",
        "        print(f\"global_step={global_step}, episodic_return={episodic_return}\")\n",
        "        writer.add_scalar(\"charts/episodic_return\", episodic_return, global_step)\n",
        "        writer.add_scalar(\"charts/episodic_length\", episodic_length, global_step)\n",
        "\n",
        "        # Optionally log to wandb\n",
        "        if args.track:\n",
        "            wandb.log({\n",
        "                \"charts/episodic_return\": episodic_return,\n",
        "                \"charts/episodic_length\": episodic_length,\n",
        "            }, step=global_step)\n",
        "            # Log video to wandb for the first environment if capture_video is true\n",
        "            if args.capture_video and idx == 0:\n",
        "                video_files = glob.glob(f\"videos/{run_name}*.mp4\")\n",
        "                if video_files:\n",
        "                    # Sort by modification time to get the latest video\n",
        "                    latest_video_file = max(video_files, key=os.path.getmtime)\n",
        "                    wandb.log({\"video\": wandb.Video(latest_video_file, fps=4, format=\"mp4\")}, step=global_step)\n",
        "\n",
        "        break  # Log only the first finished episode per step\n",
        "\n",
        "  # bootstrap value if not done\n",
        "  with torch.no_grad():\n",
        "    next_value = agent.get_value(next_obs).reshape(1,-1)\n",
        "    if args.gae:\n",
        "      advantages = torch.zeros_like(rewards).to(device)\n",
        "      lastgaelam = 0\n",
        "      for t in reversed(range(args.num_steps)):\n",
        "          if t == args.num_steps - 1:\n",
        "              nextnonterminal = 1.0 - next_done\n",
        "              nextvalues = next_value\n",
        "          else:\n",
        "              nextnonterminal = 1.0 - dones[t + 1]\n",
        "              nextvalues = values[t + 1]\n",
        "          delta = rewards[t] + nextvalues * nextnonterminal * args.gamma - values[t] # Corrected line\n",
        "          advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
        "      returns = advantages + values\n",
        "    else:\n",
        "      returns = torch.zeros_like(rewards).to(device)\n",
        "      for t in reversed(range(args.num_steps)):\n",
        "          if t == args.num_steps - 1:\n",
        "              nextnonterminal = 1.0 - next_done\n",
        "              next_return = next_value\n",
        "          else:\n",
        "              nextnonterminal = 1.0 - dones[t + 1]\n",
        "              next_return = returns[t + 1]\n",
        "          returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return\n",
        "      advantages = returns - values\n",
        "  # flatten the batch\n",
        "  b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
        "  b_logprobs = logprobs.reshape(-1)\n",
        "  b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
        "  b_advantages = advantages.reshape(-1)\n",
        "  b_returns = returns.reshape(-1)\n",
        "  b_values = values.reshape(-1)\n",
        "\n",
        "  # Optimizing the policy and value network\n",
        "  b_inds = np.arange(args.batch_size) # 512\n",
        "  clipfracs = []\n",
        "  for epoch in range(args.update_epochs):\n",
        "    np.random.shuffle(b_inds)\n",
        "    for start in range(0, args.batch_size, args.minibatch_size):\n",
        "      end = start + args.minibatch_size\n",
        "      mb_inds = b_inds[start:end]\n",
        "      _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
        "      logratio = newlogprob - b_logprobs[mb_inds]\n",
        "      ratio = logratio.exp()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
        "        old_approx_kl = (-logratio).mean()\n",
        "        approx_kl = ((ratio - 1) - logratio).mean()\n",
        "        clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
        "\n",
        "      mb_advantages = b_advantages[mb_inds]\n",
        "      if args.norm_adv:\n",
        "        mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
        "\n",
        "      # Policy loss\n",
        "      pg_loss1 = -mb_advantages * ratio\n",
        "      pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
        "      pg_loss = torch.max(pg_loss1, pg_loss2).mean() # Fixed line: added .mean() here\n",
        "\n",
        "      # Value loss\n",
        "      newvalue = newvalue.view(-1)\n",
        "      if args.clip_vloss:\n",
        "        v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
        "        v_clipped = b_values[mb_inds] + torch.clamp(\n",
        "                        newvalue - b_values[mb_inds],\n",
        "                        -args.clip_coef,\n",
        "                        args.clip_coef,\n",
        "                    )\n",
        "        v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
        "        v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
        "        v_loss = 0.5 * v_loss_max.mean()\n",
        "      else:\n",
        "        v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
        "\n",
        "      # Entropy loss\n",
        "      entropy_loss = entropy.mean()\n",
        "      # Overall loss\n",
        "      loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
        "\n",
        "      # Backprop\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      # Clip gradient\n",
        "      nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
        "      optimizer.step()\n",
        "\n",
        "    if args.target_kl is not None:\n",
        "      if approx_kl > args.target_kl:\n",
        "        break\n",
        "  y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
        "  var_y = np.var(y_true)\n",
        "  explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
        "\n",
        "  # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
        "  writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
        "  writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n",
        "  writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n",
        "  writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
        "  writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n",
        "  writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n",
        "  writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
        "  writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
        "  print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
        "  writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
        "\n",
        "envs.close()\n",
        "writer.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GSZu-9RClnVV",
        "outputId": "9301526a-13c0-402c-c4d8-d77f1ddcc55d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "global_step=60, episodic_return=15.0\n",
            "global_step=68, episodic_return=17.0\n",
            "global_step=84, episodic_return=21.0\n",
            "global_step=100, episodic_return=25.0\n",
            "global_step=152, episodic_return=16.0\n",
            "global_step=164, episodic_return=25.0\n",
            "global_step=176, episodic_return=26.0\n",
            "global_step=220, episodic_return=29.0\n",
            "global_step=224, episodic_return=11.0\n",
            "global_step=268, episodic_return=25.0\n",
            "global_step=308, episodic_return=20.0\n",
            "global_step=336, episodic_return=16.0\n",
            "global_step=356, episodic_return=50.0\n",
            "global_step=376, episodic_return=16.0\n",
            "global_step=396, episodic_return=9.0\n",
            "global_step=448, episodic_return=17.0\n",
            "global_step=464, episodic_return=31.0\n",
            "global_step=468, episodic_return=17.0\n",
            "global_step=492, episodic_return=10.0\n",
            "SPS: 581\n",
            "global_step=520, episodic_return=13.0\n",
            "global_step=556, episodic_return=15.0\n",
            "global_step=564, episodic_return=10.0\n",
            "global_step=568, episodic_return=86.0\n",
            "global_step=576, episodic_return=13.0\n",
            "global_step=644, episodic_return=16.0\n",
            "global_step=648, episodic_return=20.0\n",
            "global_step=688, episodic_return=29.0\n",
            "global_step=700, episodic_return=35.0\n",
            "global_step=704, episodic_return=14.0\n",
            "global_step=744, episodic_return=23.0\n",
            "global_step=776, episodic_return=17.0\n",
            "global_step=836, episodic_return=33.0\n",
            "global_step=860, episodic_return=28.0\n",
            "global_step=904, episodic_return=10.0\n",
            "global_step=912, episodic_return=18.0\n",
            "global_step=924, episodic_return=36.0\n",
            "global_step=928, episodic_return=16.0\n",
            "global_step=968, episodic_return=13.0\n",
            "global_step=984, episodic_return=19.0\n",
            "SPS: 576\n",
            "global_step=1044, episodic_return=29.0\n",
            "global_step=1080, episodic_return=23.0\n",
            "global_step=1112, episodic_return=16.0\n",
            "global_step=1132, episodic_return=21.0\n",
            "global_step=1144, episodic_return=15.0\n",
            "global_step=1204, episodic_return=14.0\n",
            "global_step=1240, episodic_return=31.0\n",
            "global_step=1256, episodic_return=12.0\n",
            "global_step=1276, episodic_return=35.0\n",
            "global_step=1304, episodic_return=11.0\n",
            "global_step=1320, episodic_return=19.0\n",
            "global_step=1376, episodic_return=101.0\n",
            "global_step=1384, episodic_return=19.0\n",
            "global_step=1408, episodic_return=21.0\n",
            "global_step=1472, episodic_return=23.0\n",
            "global_step=1488, episodic_return=25.0\n",
            "global_step=1492, episodic_return=28.0\n",
            "global_step=1496, episodic_return=21.0\n",
            "SPS: 725\n",
            "global_step=1560, episodic_return=15.0\n",
            "global_step=1596, episodic_return=30.0\n",
            "global_step=1600, episodic_return=27.0\n",
            "global_step=1648, episodic_return=11.0\n",
            "global_step=1668, episodic_return=26.0\n",
            "global_step=1680, episodic_return=19.0\n",
            "global_step=1728, episodic_return=32.0\n",
            "global_step=1768, episodic_return=29.0\n",
            "global_step=1820, episodic_return=37.0\n",
            "global_step=1828, episodic_return=24.0\n",
            "global_step=1864, episodic_return=45.0\n",
            "global_step=1888, episodic_return=29.0\n",
            "global_step=1932, episodic_return=25.0\n",
            "global_step=1944, episodic_return=19.0\n",
            "global_step=1984, episodic_return=12.0\n",
            "global_step=2004, episodic_return=14.0\n",
            "global_step=2016, episodic_return=17.0\n",
            "global_step=2032, episodic_return=35.0\n",
            "SPS: 831\n",
            "global_step=2068, episodic_return=20.0\n",
            "global_step=2088, episodic_return=17.0\n",
            "global_step=2100, episodic_return=16.0\n",
            "global_step=2208, episodic_return=50.0\n",
            "global_step=2216, episodic_return=31.0\n",
            "global_step=2252, episodic_return=45.0\n",
            "global_step=2348, episodic_return=32.0\n",
            "global_step=2388, episodic_return=71.0\n",
            "global_step=2440, episodic_return=22.0\n",
            "global_step=2448, episodic_return=59.0\n",
            "global_step=2520, episodic_return=19.0\n",
            "SPS: 898\n",
            "global_step=2600, episodic_return=52.0\n",
            "global_step=2696, episodic_return=23.0\n",
            "global_step=2740, episodic_return=72.0\n",
            "global_step=2756, episodic_return=14.0\n",
            "global_step=2808, episodic_return=91.0\n",
            "global_step=2816, episodic_return=53.0\n",
            "global_step=2888, episodic_return=32.0\n",
            "global_step=2916, episodic_return=26.0\n",
            "global_step=2924, episodic_return=45.0\n",
            "global_step=2944, episodic_return=31.0\n",
            "global_step=2980, episodic_return=13.0\n",
            "global_step=3024, episodic_return=33.0\n",
            "global_step=3036, episodic_return=29.0\n",
            "SPS: 773\n",
            "global_step=3092, episodic_return=27.0\n",
            "global_step=3112, episodic_return=41.0\n",
            "global_step=3120, episodic_return=23.0\n",
            "global_step=3128, episodic_return=22.0\n",
            "global_step=3192, episodic_return=15.0\n",
            "global_step=3232, episodic_return=29.0\n",
            "global_step=3240, episodic_return=36.0\n",
            "global_step=3248, episodic_return=31.0\n",
            "global_step=3276, episodic_return=20.0\n",
            "global_step=3308, episodic_return=16.0\n",
            "global_step=3328, episodic_return=12.0\n",
            "global_step=3344, episodic_return=27.0\n",
            "global_step=3352, episodic_return=25.0\n",
            "global_step=3368, episodic_return=14.0\n",
            "global_step=3392, episodic_return=15.0\n",
            "global_step=3436, episodic_return=22.0\n",
            "global_step=3500, episodic_return=36.0\n",
            "global_step=3560, episodic_return=47.0\n",
            "global_step=3572, episodic_return=44.0\n",
            "SPS: 830\n",
            "global_step=3624, episodic_return=12.0\n",
            "global_step=3648, episodic_return=52.0\n",
            "global_step=3680, episodic_return=44.0\n",
            "global_step=3728, episodic_return=25.0\n",
            "global_step=3764, episodic_return=50.0\n",
            "global_step=3772, episodic_return=30.0\n",
            "global_step=3800, episodic_return=17.0\n",
            "global_step=3848, episodic_return=18.0\n",
            "global_step=3876, episodic_return=48.0\n",
            "global_step=3896, episodic_return=32.0\n",
            "global_step=3916, episodic_return=16.0\n",
            "global_step=3948, episodic_return=17.0\n",
            "global_step=3980, episodic_return=44.0\n",
            "global_step=3996, episodic_return=24.0\n",
            "global_step=4020, episodic_return=25.0\n",
            "global_step=4068, episodic_return=29.0\n",
            "SPS: 879\n",
            "global_step=4108, episodic_return=31.0\n",
            "global_step=4208, episodic_return=24.0\n",
            "global_step=4212, episodic_return=35.0\n",
            "global_step=4216, episodic_return=54.0\n",
            "global_step=4296, episodic_return=19.0\n",
            "global_step=4324, episodic_return=28.0\n",
            "global_step=4424, episodic_return=31.0\n",
            "global_step=4464, episodic_return=34.0\n",
            "global_step=4492, episodic_return=69.0\n",
            "global_step=4536, episodic_return=27.0\n",
            "global_step=4544, episodic_return=19.0\n",
            "SPS: 922\n",
            "global_step=4636, episodic_return=153.0\n",
            "global_step=4648, episodic_return=38.0\n",
            "global_step=4656, episodic_return=29.0\n",
            "global_step=4716, episodic_return=42.0\n",
            "global_step=4760, episodic_return=27.0\n",
            "global_step=4772, episodic_return=33.0\n",
            "global_step=4792, episodic_return=33.0\n",
            "global_step=4848, episodic_return=32.0\n",
            "global_step=4852, episodic_return=22.0\n",
            "global_step=4888, episodic_return=23.0\n",
            "global_step=4912, episodic_return=15.0\n",
            "global_step=4940, episodic_return=21.0\n",
            "global_step=4952, episodic_return=24.0\n",
            "global_step=4992, episodic_return=19.0\n",
            "global_step=5032, episodic_return=22.0\n",
            "global_step=5036, episodic_return=36.0\n",
            "global_step=5076, episodic_return=20.0\n",
            "SPS: 959\n",
            "global_step=5140, episodic_return=25.0\n",
            "global_step=5192, episodic_return=59.0\n",
            "global_step=5256, episodic_return=44.0\n",
            "global_step=5312, episodic_return=42.0\n",
            "global_step=5348, episodic_return=22.0\n",
            "global_step=5396, episodic_return=90.0\n",
            "global_step=5416, episodic_return=25.0\n",
            "global_step=5492, episodic_return=18.0\n",
            "global_step=5500, episodic_return=37.0\n",
            "global_step=5528, episodic_return=32.0\n",
            "global_step=5532, episodic_return=84.0\n",
            "global_step=5624, episodic_return=23.0\n",
            "SPS: 990\n",
            "global_step=5652, episodic_return=37.0\n",
            "global_step=5664, episodic_return=42.0\n",
            "global_step=5776, episodic_return=30.0\n",
            "global_step=5852, episodic_return=56.0\n",
            "global_step=5860, episodic_return=20.0\n",
            "global_step=5868, episodic_return=83.0\n",
            "global_step=5964, episodic_return=25.0\n",
            "global_step=6004, episodic_return=84.0\n",
            "global_step=6084, episodic_return=19.0\n",
            "global_step=6132, episodic_return=69.0\n",
            "SPS: 1020\n",
            "global_step=6176, episodic_return=52.0\n",
            "global_step=6208, episodic_return=30.0\n",
            "global_step=6344, episodic_return=33.0\n",
            "global_step=6376, episodic_return=41.0\n",
            "global_step=6388, episodic_return=63.0\n",
            "global_step=6456, episodic_return=27.0\n",
            "global_step=6480, episodic_return=75.0\n",
            "global_step=6508, episodic_return=29.0\n",
            "global_step=6556, episodic_return=44.0\n",
            "global_step=6640, episodic_return=39.0\n",
            "SPS: 1047\n",
            "global_step=6664, episodic_return=51.0\n",
            "global_step=6752, episodic_return=21.0\n",
            "global_step=6788, episodic_return=69.0\n",
            "global_step=6880, episodic_return=80.0\n",
            "global_step=6996, episodic_return=60.0\n",
            "global_step=7008, episodic_return=54.0\n",
            "global_step=7012, episodic_return=92.0\n",
            "global_step=7096, episodic_return=21.0\n",
            "global_step=7128, episodic_return=61.0\n",
            "SPS: 1073\n",
            "global_step=7176, episodic_return=44.0\n",
            "global_step=7192, episodic_return=44.0\n",
            "global_step=7248, episodic_return=37.0\n",
            "global_step=7396, episodic_return=66.0\n",
            "global_step=7428, episodic_return=44.0\n",
            "global_step=7508, episodic_return=27.0\n",
            "global_step=7536, episodic_return=34.0\n",
            "global_step=7560, episodic_return=32.0\n",
            "global_step=7644, episodic_return=112.0\n",
            "SPS: 1096\n",
            "global_step=7716, episodic_return=44.0\n",
            "global_step=7780, episodic_return=67.0\n",
            "global_step=7852, episodic_return=72.0\n",
            "global_step=7932, episodic_return=37.0\n",
            "global_step=7940, episodic_return=21.0\n",
            "global_step=8004, episodic_return=55.0\n",
            "global_step=8028, episodic_return=77.0\n",
            "global_step=8076, episodic_return=17.0\n",
            "global_step=8120, episodic_return=46.0\n",
            "global_step=8168, episodic_return=22.0\n",
            "SPS: 1109\n",
            "global_step=8216, episodic_return=68.0\n",
            "global_step=8312, episodic_return=70.0\n",
            "global_step=8372, episodic_return=50.0\n",
            "global_step=8376, episodic_return=63.0\n",
            "global_step=8408, episodic_return=47.0\n",
            "global_step=8460, episodic_return=20.0\n",
            "global_step=8472, episodic_return=24.0\n",
            "global_step=8520, episodic_return=51.0\n",
            "SPS: 1034\n",
            "global_step=8728, episodic_return=79.0\n",
            "global_step=8800, episodic_return=69.0\n",
            "global_step=8884, episodic_return=105.0\n",
            "global_step=8932, episodic_return=114.0\n",
            "global_step=8936, episodic_return=51.0\n",
            "global_step=8960, episodic_return=39.0\n",
            "global_step=8992, episodic_return=13.0\n",
            "global_step=9048, episodic_return=40.0\n",
            "global_step=9112, episodic_return=44.0\n",
            "global_step=9172, episodic_return=44.0\n",
            "global_step=9208, episodic_return=39.0\n",
            "SPS: 1043\n",
            "global_step=9292, episodic_return=44.0\n",
            "global_step=9432, episodic_return=64.0\n",
            "global_step=9484, episodic_return=130.0\n",
            "global_step=9508, episodic_return=53.0\n",
            "global_step=9568, episodic_return=89.0\n",
            "global_step=9636, episodic_return=37.0\n",
            "global_step=9644, episodic_return=18.0\n",
            "global_step=9700, episodic_return=47.0\n",
            "SPS: 1050\n",
            "global_step=9752, episodic_return=79.0\n",
            "global_step=9848, episodic_return=52.0\n",
            "global_step=9900, episodic_return=49.0\n",
            "global_step=9940, episodic_return=73.0\n",
            "global_step=10044, episodic_return=48.0\n",
            "global_step=10084, episodic_return=35.0\n",
            "global_step=10136, episodic_return=95.0\n",
            "global_step=10180, episodic_return=69.0\n",
            "global_step=10204, episodic_return=39.0\n",
            "SPS: 1059\n",
            "global_step=10308, episodic_return=25.0\n",
            "global_step=10376, episodic_return=59.0\n",
            "global_step=10380, episodic_return=49.0\n",
            "global_step=10404, episodic_return=79.0\n",
            "global_step=10568, episodic_return=46.0\n",
            "global_step=10652, episodic_return=85.0\n",
            "SPS: 1068\n",
            "global_step=10760, episodic_return=95.0\n",
            "global_step=10828, episodic_return=105.0\n",
            "global_step=10880, episodic_return=29.0\n",
            "global_step=10892, episodic_return=59.0\n",
            "global_step=10992, episodic_return=40.0\n",
            "global_step=11024, episodic_return=113.0\n",
            "global_step=11036, episodic_return=38.0\n",
            "global_step=11196, episodic_return=50.0\n",
            "global_step=11264, episodic_return=92.0\n",
            "SPS: 1073\n",
            "global_step=11368, episodic_return=85.0\n",
            "global_step=11460, episodic_return=65.0\n",
            "global_step=11476, episodic_return=109.0\n",
            "global_step=11500, episodic_return=58.0\n",
            "global_step=11512, episodic_return=35.0\n",
            "global_step=11612, episodic_return=24.0\n",
            "global_step=11636, episodic_return=33.0\n",
            "global_step=11740, episodic_return=65.0\n",
            "SPS: 1076\n",
            "global_step=11792, episodic_return=38.0\n",
            "global_step=11800, episodic_return=46.0\n",
            "global_step=11804, episodic_return=85.0\n",
            "global_step=11924, episodic_return=45.0\n",
            "global_step=11940, episodic_return=36.0\n",
            "global_step=12004, episodic_return=50.0\n",
            "global_step=12020, episodic_return=23.0\n",
            "global_step=12084, episodic_return=69.0\n",
            "global_step=12200, episodic_return=44.0\n",
            "global_step=12232, episodic_return=56.0\n",
            "SPS: 1078\n",
            "global_step=12320, episodic_return=94.0\n",
            "global_step=12332, episodic_return=24.0\n",
            "global_step=12384, episodic_return=74.0\n",
            "global_step=12452, episodic_return=62.0\n",
            "global_step=12504, episodic_return=42.0\n",
            "global_step=12516, episodic_return=48.0\n",
            "global_step=12580, episodic_return=18.0\n",
            "global_step=12736, episodic_return=87.0\n",
            "SPS: 1092\n",
            "global_step=12824, episodic_return=92.0\n",
            "global_step=12836, episodic_return=79.0\n",
            "global_step=13048, episodic_return=55.0\n",
            "global_step=13092, episodic_return=63.0\n",
            "global_step=13160, episodic_return=16.0\n",
            "global_step=13208, episodic_return=39.0\n",
            "global_step=13288, episodic_return=19.0\n",
            "SPS: 1106\n",
            "global_step=13316, episodic_return=119.0\n",
            "global_step=13344, episodic_return=45.0\n",
            "global_step=13368, episodic_return=196.0\n",
            "global_step=13468, episodic_return=44.0\n",
            "global_step=13500, episodic_return=38.0\n",
            "global_step=13540, episodic_return=55.0\n",
            "global_step=13592, episodic_return=30.0\n",
            "global_step=13660, episodic_return=39.0\n",
            "global_step=13744, episodic_return=50.0\n",
            "global_step=13788, episodic_return=104.0\n",
            "SPS: 1114\n",
            "global_step=13836, episodic_return=60.0\n",
            "global_step=13964, episodic_return=43.0\n",
            "global_step=14032, episodic_return=48.0\n",
            "global_step=14176, episodic_return=128.0\n",
            "global_step=14248, episodic_return=125.0\n",
            "global_step=14336, episodic_return=75.0\n",
            "SPS: 1125\n",
            "global_step=14508, episodic_return=82.0\n",
            "global_step=14520, episodic_return=138.0\n",
            "global_step=14536, episodic_return=71.0\n",
            "global_step=14688, episodic_return=87.0\n",
            "global_step=14760, episodic_return=62.0\n",
            "global_step=14808, episodic_return=67.0\n",
            "SPS: 1137\n",
            "global_step=14968, episodic_return=69.0\n",
            "global_step=14980, episodic_return=114.0\n",
            "global_step=14988, episodic_return=56.0\n",
            "global_step=15064, episodic_return=63.0\n",
            "global_step=15108, episodic_return=31.0\n",
            "global_step=15144, episodic_return=38.0\n",
            "global_step=15308, episodic_return=84.0\n",
            "SPS: 1147\n",
            "global_step=15376, episodic_return=57.0\n",
            "global_step=15484, episodic_return=93.0\n",
            "global_step=15500, episodic_return=47.0\n",
            "global_step=15684, episodic_return=154.0\n",
            "global_step=15796, episodic_return=77.0\n",
            "global_step=15852, episodic_return=118.0\n",
            "SPS: 1157\n",
            "global_step=15904, episodic_return=54.0\n",
            "global_step=15908, episodic_return=101.0\n",
            "global_step=15964, episodic_return=27.0\n",
            "global_step=16036, episodic_return=31.0\n",
            "global_step=16048, episodic_return=62.0\n",
            "global_step=16212, episodic_return=76.0\n",
            "global_step=16296, episodic_return=61.0\n",
            "global_step=16344, episodic_return=94.0\n",
            "SPS: 1167\n",
            "global_step=16560, episodic_return=130.0\n",
            "global_step=16608, episodic_return=98.0\n",
            "global_step=16612, episodic_return=66.0\n",
            "global_step=16660, episodic_return=90.0\n",
            "global_step=16852, episodic_return=60.0\n",
            "global_step=16856, episodic_return=73.0\n",
            "SPS: 1176\n",
            "global_step=17000, episodic_return=35.0\n",
            "global_step=17048, episodic_return=96.0\n",
            "global_step=17052, episodic_return=109.0\n",
            "global_step=17124, episodic_return=18.0\n",
            "global_step=17136, episodic_return=33.0\n",
            "global_step=17256, episodic_return=100.0\n",
            "global_step=17392, episodic_return=84.0\n",
            "SPS: 1185\n",
            "global_step=17548, episodic_return=105.0\n",
            "global_step=17576, episodic_return=109.0\n",
            "global_step=17644, episodic_return=62.0\n",
            "global_step=17752, episodic_return=43.0\n",
            "global_step=17760, episodic_return=125.0\n",
            "global_step=17836, episodic_return=47.0\n",
            "SPS: 1194\n",
            "global_step=17976, episodic_return=106.0\n",
            "global_step=18064, episodic_return=56.0\n",
            "global_step=18104, episodic_return=87.0\n",
            "global_step=18180, episodic_return=18.0\n",
            "global_step=18240, episodic_return=43.0\n",
            "global_step=18396, episodic_return=158.0\n",
            "SPS: 1199\n",
            "global_step=18444, episodic_return=116.0\n",
            "global_step=18460, episodic_return=69.0\n",
            "global_step=18532, episodic_return=33.0\n",
            "global_step=18712, episodic_return=62.0\n",
            "global_step=18744, episodic_return=52.0\n",
            "global_step=18776, episodic_return=133.0\n",
            "global_step=18792, episodic_return=86.0\n",
            "global_step=18904, episodic_return=39.0\n",
            "SPS: 1207\n",
            "global_step=19120, episodic_return=81.0\n",
            "global_step=19252, episodic_return=118.0\n",
            "global_step=19340, episodic_return=156.0\n",
            "global_step=19416, episodic_return=127.0\n",
            "SPS: 1215\n",
            "global_step=19476, episodic_return=33.0\n",
            "global_step=19560, episodic_return=35.0\n",
            "global_step=19584, episodic_return=82.0\n",
            "global_step=19704, episodic_return=56.0\n",
            "global_step=19868, episodic_return=40.0\n",
            "global_step=19872, episodic_return=71.0\n",
            "global_step=19956, episodic_return=98.0\n",
            "SPS: 1223\n",
            "global_step=19988, episodic_return=216.0\n",
            "global_step=20096, episodic_return=55.0\n",
            "global_step=20180, episodic_return=77.0\n",
            "global_step=20184, episodic_return=48.0\n",
            "global_step=20380, episodic_return=105.0\n",
            "global_step=20412, episodic_return=78.0\n",
            "global_step=20476, episodic_return=72.0\n",
            "SPS: 1229\n",
            "global_step=20508, episodic_return=81.0\n",
            "global_step=20652, episodic_return=67.0\n",
            "global_step=20912, episodic_return=124.0\n",
            "global_step=20932, episodic_return=69.0\n",
            "global_step=20940, episodic_return=115.0\n",
            "SPS: 1236\n",
            "global_step=21064, episodic_return=138.0\n",
            "global_step=21160, episodic_return=23.0\n",
            "global_step=21212, episodic_return=69.0\n",
            "global_step=21276, episodic_return=90.0\n",
            "global_step=21324, episodic_return=27.0\n",
            "global_step=21332, episodic_return=97.0\n",
            "SPS: 1243\n",
            "global_step=21528, episodic_return=62.0\n",
            "global_step=21636, episodic_return=118.0\n",
            "global_step=21640, episodic_return=76.0\n",
            "global_step=21696, episodic_return=41.0\n",
            "global_step=21800, episodic_return=118.0\n",
            "global_step=21856, episodic_return=39.0\n",
            "SPS: 1248\n",
            "global_step=22044, episodic_return=60.0\n",
            "global_step=22056, episodic_return=104.0\n",
            "global_step=22172, episodic_return=132.0\n",
            "global_step=22316, episodic_return=64.0\n",
            "global_step=22392, episodic_return=133.0\n",
            "global_step=22416, episodic_return=24.0\n",
            "global_step=22512, episodic_return=84.0\n",
            "global_step=22524, episodic_return=26.0\n",
            "SPS: 1255\n",
            "global_step=22560, episodic_return=128.0\n",
            "global_step=22648, episodic_return=63.0\n",
            "global_step=22756, episodic_return=48.0\n",
            "global_step=22896, episodic_return=95.0\n",
            "global_step=22900, episodic_return=93.0\n",
            "global_step=22908, episodic_return=37.0\n",
            "global_step=22992, episodic_return=22.0\n",
            "SPS: 1262\n",
            "global_step=23180, episodic_return=69.0\n",
            "global_step=23260, episodic_return=87.0\n",
            "global_step=23488, episodic_return=123.0\n",
            "global_step=23512, episodic_return=82.0\n",
            "SPS: 1268\n",
            "global_step=23576, episodic_return=78.0\n",
            "global_step=23644, episodic_return=38.0\n",
            "global_step=23656, episodic_return=35.0\n",
            "global_step=23716, episodic_return=204.0\n",
            "global_step=23928, episodic_return=87.0\n",
            "global_step=23936, episodic_return=54.0\n",
            "global_step=24020, episodic_return=22.0\n",
            "global_step=24024, episodic_return=94.0\n",
            "global_step=24060, episodic_return=100.0\n",
            "SPS: 1273\n",
            "global_step=24252, episodic_return=47.0\n",
            "global_step=24256, episodic_return=58.0\n",
            "global_step=24360, episodic_return=105.0\n",
            "SPS: 1280\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'RecordVideo' object has no attribute 'enabled'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1165510384.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m   \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"charts/SPS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m \u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/vector/vector_env.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_extras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gymnasium/vector/sync_vector_env.py\u001b[0m in \u001b[0;36mclose_extras\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;34m\"\"\"Close the environments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"envs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/integration/gym/__init__.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mrecorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RecordVideo' object has no attribute 'enabled'"
          ]
        }
      ]
    }
  ]
}